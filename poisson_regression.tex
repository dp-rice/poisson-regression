\documentclass[12pt, letterpaper]{article}
\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{ \lambtot }{\lambda_{\text{tot}}}
\DeclareMathOperator{\cov}{cov}

\title{Poisson regression theory for exponential growth detection}
\author{Daniel P. Rice}
\date{December 2022}

\begin{document}

\maketitle

\begin{abstract}
    Objectives:
    \begin{enumerate}
        \item Sufficient generality so that we can see general properties and extensions
        \item Sufficient concreteness that we can calculate things
        \item Build intuition for scaling
        \item Compare approaches
        \item Simple model as baseline for complications
    \end{enumerate}
\end{abstract}

\tableofcontents

\section{Model}

We are interested in estimating the abundance over time, $a(t)$, of a virus circulating in the population.
Our observations consist of $n$ metagenomic samples from the environment (e.g., wastewater) taken at a discrete set of times $\left\{t_1, \ldots, t_n\right\}$.
We process the metagenomic samples to identify all k-mers of a given length and count their occurrence in each sample.
Here we focus on the counts, $X = \left\{X_1, \ldots, X_n\right\}$, of a single k-mer derived from our virus of interest.
At each sampling time $t_i$, we have a known sampling effort $\lambda_i$.
The $\left\{\lambda_i\right\}$ represent a conversion factor from the abundance of the virus to the count of the focal k-mer so that $\mathbb{E}\left[X_i\right] = \lambda_i a(t_i)$,
accounting for the combined effects of sequencing depth, extraction and sequencing efficiency, and our bioinformatics pipeline.
(Caveat: in practice the conversion factor is not fully known and may fluctuate due to compositional effects in the sample. However, it is reasonable to think of it as proportial to the sequencing depth.)

In the following we will model the counts, conditional on $a(t)$ and $\lambda$, as independent Poisson random variables.
This assumes that each copy of the virus in the population has a small probability of contributing a read to the metagenomic pool and that they do so independently of one another.
Violations of these assumptions will result in overdispersion of the counts relative to the Poisson distribution.
Exploring the consequences of overdispersion for our methods will be the subject of another document.

Assuming that $X_i \sim \text{Poisson}\left(\lambda_i a(t_i)\right)$ and the $\left\{X_i\right\}$ are mutually independent, the log-likelihood of our model is:
\begin{equation}
    l(a; x, \lambda) = \sum_i \left\{\log(a(t_i)) x_i - \lambda_i a(t_i) \right\} + \text{const.}
    \label{eq:likelihood}
\end{equation}
where the constant term depends on $x$ and $\lambda$ but not on $a$.
Eq.~\ref{eq:likelihood} is an exponential family with $n$ natural parameters $\left\{\log(a(t_i))\right\}$ and sufficient statistics $\left\{x_i\right\}$.
Fitting this model would amount to estimating the abundance at each time using only the count data at that same time.
There are two problems with this.
First, scientifically, we are not interested only in the abundances at the observed times.
Instead, we'd like to \emph{characterize} the function $a(t)$, answering questions like, ``Is the virus spreading?'' or, ``Does it fluctuate cyclically?''
Alternately, we might want to \emph{predict} the abundance in the future.
Second, the model overfits to the Poisson counting noise at each time.
As long as our sampling times aren't too widely spaced, we expect the abundance at nearby timepoints to be similar.
Accordingly, we'd like to borrow information from multiple timepoints to inform our estimate of $a(t)$.
This borrowing is the essence of regression.

In order to link our estimates of the abundance at different times, we expand $\log(a(t))$ in terms of a set of basis functions $\left\{b_r(t)\right\}$:
\begin{equation}
    \log(a(t)) = \sum_r \beta_r b_r(t).
    \label{eq:basis}
\end{equation}
For standard Poisson regression, $b_0(t) = 1$ and $b_1(t) = t$.
Here, we will work with a general basis as much as possible.
This will show us which properties depend on linearity of $\log(a(t))$ and which do not.
It will also allow us to accommodate models with periodic fluctuations.
Finally, it will show us how to select the basis with the best statistical properties among a set of equivalent parameterizations.

Substituting Eq.~\ref{eq:basis} into Eq.~\ref{eq:likelihood}, our likelihood equation becomes
\begin{equation}
    l(\beta; x, \lambda) = \sum_r \beta_r \sum_i b_r(t_i) x_i - \sum_i \lambda_i e^{\sum_r \beta_r b_r(t_i)} + \text{const.}
\end{equation}
We can now see why we chose to expand $\log(a)$ rather than $a$:
with this choice, our coefficients, $\beta$, are the natural parameters of an exponential family with sufficient statistics
\begin{equation}
    T_r(X) = \sum_i b_r(t_i) X_i
    \label{eq:suff}
\end{equation}
and log-partition function
\begin{equation}
    A(\beta) = \sum_i \lambda_i e^{\sum_r \beta_r b_r(t_i)}.
\end{equation}
(It also lets us avoid the complications of restricting the abundances to be positive.)
Note that the sufficient statistics (Eq.~\ref{eq:suff}) are linear combinations of the $X_i$, namely their projections onto the basis functions $b_r(t)$.

In the following sections, we will derive statistical procedures for inferring the coefficients $\beta$, first in a maximum likelihood framework, then in a Bayesian one.
We will see how to select basis functions and how the results depend on the sampling times and sequencing effort.
Finally, we will analyze an edge case that maximum likelihood does not handle well and show how Bayesian inference regularizes our estimates.

\section{Maximum likelihood inference}

\subsection{Maximum likelihood parameter estimates}

For an exponential family with natural parameters $\nu$, sufficient statistics $T(X)$, and log-partition function $A(\nu)$, the log likelihood is
\begin{equation}
    l(\nu; x) = \nu \cdot T(x) - A(\nu).
\end{equation}
Setting $\nabla l = 0$, we find the maximum likelihood parameters $\hat{\nu}$ satisfy the equation
\begin{align}
    T(x) &= \nabla A(\hat{\nu}) \\
         &= \mathbb{E}\left[T(X)|\hat{\nu}\right].
\end{align}
That is, $\hat{\nu}$ are the parameters for which the expected values of the sufficient statistics equal their observed values.
(The simplicity of this equation is one of the reasons to work with natural parameters.)

Applying this result to our Poisson regression model, we have
\begin{align}
    \frac{\partial A}{\partial \beta_r}(\hat{\beta}) &= T_r(x) \\
    \sum_i \lambda_i b_r(t_i) e^{\sum_{r'} \hat{\beta}_{r'} b_{r'}(t_i)}
                                                     &= \sum_i b_r(t_i) x_i.
    \label{eq:ml}
\end{align}
Eq.~\ref{eq:ml} is an implicit equation for $\hat{\beta}$, so we typically can't solve it in closed form.
Instead, we'll use Newton's method, or equivalently iterated reweighted least-squares (IRLS), to find $\hat{\beta}$ as a function of $x$.

Typically, we will want to compare our full model to a null model in which the abundance is constant over time.
In this model $b_0(t) = 1$, $\beta_r = 0$ for $r > 0$, and our task is to find $\hat{\beta}_0$.
\begin{align}
    \sum_i \lambda_i e^{\hat{\beta}_0} &= \sum_i x_i \\
    e^{\hat{\beta}_0} &= \frac{\sum_i x_i}{\lambtot},
\end{align}
where $\lambtot = \sum_i \lambda_i$ is the total sequencing effort.
This is reasonable because our null model for the abundance is $a(t) = e^{\hat{\beta}_0}$.

\subsection{Quantifying uncertainty: Fisher information}
% Definition of I
The maximum likelihood estimator $\hat{\beta}$ represents a point estimate of our parameters.
We would like to complement this with a measure of the uncertainty of this estimate.
A common measure in likelihood theory is the Fisher information matrix, defined as
\begin{equation}
    \mathcal{I}(\theta) = - \mathbb{E}\left[H_{l}(\theta; X) \middle| \theta \right],
\end{equation}
where $H_l$ is the Hessian matrix of the log likelihood function and $\theta$ are parameters.
The Fisher information represents the curvature of the log likelihood around the parameter estimate $\theta$.
Large Fisher information at the maximum likelihood estimate means that the data give a lot of information about the parameters and the estimate is sharp.
Asymptotically, $\mathcal{I}^{-1}$ is the covariance matrix of the maximum likelihood estimator.

For an exponential family with natural parameters $\nu$, the Hessian is independent of the data and we have:
\begin{align}
    \mathcal{I}_{r,s}(\nu) &= \frac{\partial^2}{\partial \nu_r \partial \nu_s} A(\nu)\\
                           &= \cov\left(T_r(X), T_s(X)\right).
\end{align}
In our Poisson regression model, this gives
\begin{align}
    \mathcal{I}_{r,s}(\beta) &= \sum_i \lambda_i b_r(t_i) b_s(t_i) e^{\sum_{r'} \beta_{r'} b_{r'}(t_i)}.
    \label{eq:fisher_info}
\end{align}
Under the null model, this reduces to
\begin{align}
    \mathcal{I}_{r,s}(\beta_0) &= e^{\beta_0} \sum_i \lambda_i b_r(t_i) b_s(t_i) \\
                               &= e^{\beta_0} \lambtot \left< b_r, b_s \right>_\lambda. \label{eq:inner_prod}
\end{align}
Eq.~\ref{eq:inner_prod} represents the sum in the previous line as an inner product of the basis functions $b_r$ and $b_s$ with respect to the normalized weights $\lambda / \lambtot$.
Note that the Fisher information increases proportionally to the expected total count.

\subsection{Choosing basis functions}

Eq.~\ref{eq:inner_prod} suggests that we should choose our basis functions so that they are orthogonal with respect to the $\lambda$ weights:
\begin{equation}
    \mathcal{I}_{r,s}(\beta_0) = e^{\beta_0} \lambtot \| b_r \|_{\lambda}^2 \delta_{r,s}
    \label{eq:i_ortho}
\end{equation}
Such an orthogonal basis will produce a diagonal Fisher information matrix under the null, which will simplify our analytical methods and make our numerical methods better conditioned.

For exponential growth detection, we are interested in a polynomial basis truncated at first order.
That is,
\begin{align}
    b_0(t) &= 1 \\
    b_1(t) &= t - c
\end{align}
where $c$ is a constant to be determined.
(We also have a coefficient multiplying $t$, but it is convenient to set it to be 1 so that $\beta_1$ is the growth rate).
We choose $c$ to satisfy the orthogonality condition:
\begin{align}
    \left<b_0, b_1\right>_\lambda &= 0 \\
    \sum_i \frac{\lambda_i}{\lambtot} (t_i - c) &= 0 \\
    c &= \sum_i \frac{\lambda_i}{\lambtot} t_i.
\end{align}
Recognizing that the right-hand side of the last line is the average sampling time, weighted by the sampling intensity, we label it $\bar{t}$, so that
\begin{align}
    b_1(t) = t - \bar{t}.
\end{align}

With this choice of basis, our sufficient statistics and log-partition function are:
\begin{align}
    T_0(X) &= \sum_i X_i \label{eq:t0} \\
    T_1(X) &= \sum_i (t_i - \bar{t}) X_i \label{eq:t1} \\
    A(\beta) &= \sum_i \lambda_i e^{\beta_0 + \beta_1 (t - \bar{t})}.
\end{align}
Eqs.~\ref{eq:t0}--\ref{eq:t1} show that our sufficient statistics are the total observed counts, and the sum of observed counts weighted by the time they were observed.

Our ML equations (Eq.~\ref{eq:ml}) become:
\begin{align}
    \sum_i \lambda_i e^{\hat{\beta}_0 + \hat{\beta}_1 (t_i - \bar{t})} &= \sum_i x_i, \label{eq:ml0} \\
    \sum_i \lambda_i (t_i - \bar{t}) e^{\hat{\beta}_0 + \hat{\beta}_1 (t_i - \bar{t})} &= \sum_i (t_i - \bar{t}) x_i. \label{eq:ml1}
\end{align}
Assuming that we observe at least one count so that $T_0(x) > 0$, we can divide Eq.~\ref{eq:ml1} by Eq.~\ref{eq:ml0} to eliminate $\hat{\beta}_0$ and get an implicit equation for $\hat{\beta}_1$:
\begin{align}
    \frac{\sum_i \lambda_i (t_i - \bar{t}) e^{\hat{\beta}_1 (t_i - \bar{t})}}{\sum_i \lambda_i e^{\hat{\beta}_1 (t_i - \bar{t})}}
    &= \frac{T_1(x)}{T_0(x)}.
    \label{eq:ml_ratio}
\end{align}
That is, we can compute the maximum likelihood estimator of the growth rate from the time-weighted sum of counts $T_1$, normalized by the total count $T_0$.

To find the Fisher information under the null, all that remains is to find the norms of the basis functions
\begin{align}
    \|b_0\| &= 1, \\
    \|b_1\| &= \sum_i \frac{\lambda_i}{\lambtot} {(t_i - \bar{t})}^2 = \sigma_t^2,
\end{align}
which  we can substitute into Eq.~\ref{eq:i_ortho} to get:
\begin{align}
    \mathcal{I}(\beta_0) = e^{\beta_0} \lambtot
    \begin{pmatrix}
        1 & 0 \\
        0 & \sigma_t^2
    \end{pmatrix}.
\end{align}
This confirms the intuition that our ability to estimate that non-growing virus are not growing increases with sampling depth and the dispersion of our samples in time.

\subsection{Fisher information under the alternative hypothesis}

So far we have only calculated the Fisher information under the null hypothesis $\beta_r = 0$ for $r > 0$.
This is a useful baseline, but if we want a better measure of our uncertainty in our estimates, we also need the Fisher information under the alternative hypothesis.
In our chosen orthogonal basis, Eq.~\ref{eq:fisher_info} becomes
\begin{align}
    \mathcal{I}_{0,0} &= \sum_i \lambda_i e^{\hat{\beta}_0 + \hat{\beta}_1 (t_i - \bar{t})}
                      &= T_0(x) \label{eq:i00} \\
    \mathcal{I}_{0,1} = \mathcal{I}_{1,0} &= \sum_i \lambda_i (t_i - \bar{t}) e^{\hat{\beta}_0 + \hat{\beta}_1 (t_i - \bar{t})}
                                          &= T_1(x) \label{eq:i01} \\
    \mathcal{I}_{1,1} &= \sum_i \lambda_i {(t_i - \bar{t})}^2 e^{\hat{\beta}_0 + \hat{\beta}_1 (t_i - \bar{t})} \label{eq:i11}
\end{align}
The second equalities in Eqs.~\ref{eq:i00} and~\ref{eq:i01} are due to the maximum likelihood equations (\ref{eq:ml0}--\ref{eq:ml1}).
(They derive from the fact that $\frac{\partial A}{\partial \beta_0} = A$.)

Eq.~\ref{eq:i11} does not have a simple relationship to the sufficient statistics for general $\lambda$.
However, we can gain some insight by considering the ratio $\mathcal{I}_{1,1} / T_0(x)$ in the limits of small and large $\hat{\beta}_0$.
First, note that this ratio is independent of $\hat{\beta}_0$.
\begin{align}
    \frac{\mathcal{I}_{1,1}}{T_0(x)} &= \frac
        {\sum_i \lambda_i {(t_i - \bar{t})}^2 e^{\hat{\beta}_0 + \hat{\beta}_1 (t_i - \bar{t})}}
        {\sum_i \lambda_i e^{\hat{\beta}_0 + \hat{\beta}_1 (t_i - \bar{t})}} \\
                                     &= \frac
        {\sum_i \lambda_i {(t_i - \bar{t})}^2 e^{\hat{\beta}_1 (t_i - \bar{t})}}
        {\sum_i \lambda_i e^{\hat{\beta}_1 (t_i - \bar{t})}}
\end{align}

in the limit $\hat{\beta}_1 \to 0$, we can expand in a power series:
\begin{align}
    \frac{\mathcal{I}_{1,1}}{t_0(x)} &= \frac
        {\sum_i \lambda_i {(t_i - \bar{t})}^2 \left(1 + \hat{\beta}_1 (t_i - \bar{t}) + \frac{1}{2}\hat{\beta}_1^2 {(t_i - \bar{t})}^2 + O({\hat{\beta}_1}^3)\right)}
        {\sum_i \lambda_i \left(1 + \hat{\beta}_1 (t_i - \bar{t}) + \frac{1}{2}\hat{\beta}_1^2{(t_i - \bar{t})}^2 + O({\hat{\beta}_1}^3)\right)} \\
                                     &= \sigma_t^2 + \mu_t^{(3)} \hat{\beta}_1 + (\mu_t^{(4)} - \sigma_t^4) \hat{\beta}_1^2 + O(\hat{\beta}_1^3),
\end{align}
where $\mu_t^{(k)}$ is the $k$-th central moment of the $\lambda$-weighted sampling times.

In the limit $\hat{\beta}_1 \to \infty$, the final sampling time dominates the sum and we have:
\begin{align}
    \frac{\mathcal{I}_{1,1}}{T_0(x)} &= \frac
        {\lambda_n {(t_n - \bar{t})}^2 e^{\hat{\beta}_1 (t_n - \bar{t})} + o(e^{\hat{\beta}_1 (t_n - \bar{t})})}
        {\lambda_n e^{\hat{\beta}_1 (t_n - \bar{t})} + o(e^{\hat{\beta}_1 (t_n - \bar{t})})} \\
                                     &\to {(t_n - \bar{t})}^2.
\end{align}
That is, the ratio is bounded for large $\hat{\beta}_1$.

Putting these results together gives us that the Fisher information under the alternative hypothesis has the form
\begin{align}
    \mathcal{I}(\hat{\beta}) &=
    \begin{pmatrix}
        T_0(x) & T_1(x) \\
        T_1(x) & f(\hat{\beta}_1) T_0(x)
    \end{pmatrix}, \\
\end{align}
where
\begin{align}
    f(\hat{\beta}_1) &= \sigma_t^2 + \mu_t^{(3)} \hat{\beta}_1 + (\mu_t^{(4)} - \sigma_t^4) \hat{\beta}_1^2 + O(\hat{\beta}_1^3), & \hat{\beta}_1 \to 0, \\
                     & \to {(t_n - \bar{t})}^2, & \hat{\beta_1} \to \infty.
\end{align}
For $T_1(x) = 0$, $\hat{\beta}_1 = 0$ and we recover the null result.
For $T_1(x) > 0$, $\hat{\beta}_1 > 0$, we get that the asymptotic variance of $\hat{\beta}_1$ increases with $\hat{\beta}_1$, but plateaus.

\subsection{A problematic edge case: the first observation}

In this section, we examine an edge case that causes difficulty for maximum likelihood estimation.
Consider observing a sequence of zero counts followed by a single non-zero observation, i.e., $x = (0, \ldots, 0, x_n)$.
In the context of continuous monitoring, this is the data we'd expect the first time we observed a k-mer.

In this case, the sufficient statistics are
\begin{align}
    T_0(x) &= x_n \\
    T_1(x) &= (t_n - \bar{t}) x_n
\end{align}
Substituting into Eq.~\ref{eq:ml_ratio} gives
\begin{align}
    \frac
        {\sum_i \lambda_i (t_i - \bar{t}) e^{\hat{\beta}_1(t_i - \bar{t})}}
        {\sum_i \lambda_i e^{\hat{\beta}_1(t_i - \bar{t})}}
    & = \frac{(t_n - \bar{t}) x_n}{x_n} \\
    & = t_n - \bar{t},
\end{align}
which is solved by letting $\hat{\beta}_1 \to \infty$.
In order to match the finite $T_0$, $\hat{\beta}_0$ must go to $-\infty$ like
\begin{align}
    \hat{\beta}_1 \sim \log(x_n / \lambda_n) - (t_n - \bar{t})\hat{\beta}_1.
    \label{eq:ec_scaling}
\end{align}
As in the logistic regression when there is perfect separation between cases, the model tries to fit this data with an infinitely steep curve that passes through all the points exactly.

The Fisher information matrix is
\begin{align}
    \mathcal{I}(\hat{\beta}) &= x_n
    \begin{pmatrix}
        1 & (t_n - \bar{t}) \\
        (t_n - \bar{t}) & {(t_n - \bar{t})}^2
    \end{pmatrix}.
\end{align}
Note that all terms are finite.

Thus we have a case in which the maximum likelihood estimator takes on an extreme result based on relatively little information.
Worse, the Fisher information is a poor guide to our uncertainty about the parameter estimates.
Intuitively, there are a wide range of parameter values that are consistent with this data, but the Fisher information suggests that we should be confident that the parameters are infinite.
Regression software that outputs approximate p-values based on the Fisher information will be infinitely overconfident that the null hypothesis can be rejected.
In the next section, we'll explore hypothesis testing in more detail.

\subsection{Null hypothesis testing}

One way of framing exponential growth detection is as a test of the null hypothesis that $\hat{\beta}_1 = 0$.
In frequentist statistics a null hypothesis test is defined by a choice of \emph{test statistic}, a function of the data that is designed to quantify the degree to which the data would be surprising if the null hypothesis were true.
To apply the test, the user computes the test statistic from the observed data and uses it to calculate (or more typically approximate) a \emph{p-value}, the probability of observing a more extreme value of the test statistic under the null hypothesis.
If the p-value falls below a pre-specified threshold, the null hypothesis is rejected.
For a given problem, there are many possible choices of test statistic, which vary in their ease of calculation and ability to discriminate between models.
In this section, we'll examine the properties of three different test statistics.

A particular challenge for null hypothesis testing in the context of exponential growth detection is the multiple hypothesis testing burden.
A metagenomic sequence dataset will have a very large number of k-mers.
If we were to set a p-value threshold at a typical value of 0.05, we would expect to reject the null hypothesis for one out of every twenty k-mers, even if none were growing at all.
This scale of false positives would likely swamp our post-processing pipeline.
To reduce the number of false positives, we could set a higher p-value threshold for our test.
However, the asymptotic approximations to the null distribution of the test statistic break down in the extreme tails of the distribution, even in very large samples.
Worse, any model misspecification issues are likely to be worse in the tails as well.
Therefore, it probably makes more sense to avoid p-values and either to construct a decision procedure based directly on the test statistic, to use Bayesian regularization (see below), and/or to design a loss function based on particular concrete objectives.
(Note: any procedure we develop should take into account that the data is arriving as a stream rather than just one time.)

\subsubsection{Z-test}

The most obvious choice of test statistic in a regression context is the regression coefficient $\beta$ itself.
In particular, we could ask the probability that the maximum likelihood estimator $\hat{\beta}_1$ would be greater than inferred from the data if the true $\hat{\beta}_1 = 0$.
This is the intuition behind the Z-test, which uses the test statistic
\begin{equation}
    Z = \frac{\hat{\beta}_1}{\sqrt{\mathcal{I}_{1,1}(\hat{\beta})}}.
\end{equation}
Likehihood theory shows that in the limit of large datasets $Z$ is asymptotically normal.
When the \texttt{statsmodels} Python package reports Z-scores and p-values, this what is it is using.
(Note: we have shown that the Fisher information depends on the parameters, so that the variance of $\hat{\beta}_1$ is different under the null and alternative hypotheses.
The software uses the Fisher information evaluated at $\hat{\beta}$, which overestimates the variability in $\hat{\beta}_1$ under the null.
It would be better to normalize by $\mathcal{I}_{1,1}(\hat{\beta}_\text{null})$.)

There are a few difficulties with working with the Z-test.
First, $\hat{\beta}_1$ is a complicated, non-linear transformation of the data, making it relatively slow to calculate and impossible to get exact null distributions for $Z$.
Second, we have seen that in the edge case examined above, $\hat{\beta}_1 \to \infty$, while the Fisher information is finite.
The Z-test will thus report infinitely small p-values, even if there is only a single count of the k-mer in the dataset.
This suggests that $Z$ does a poor job of summarizing the strength of evidence against the null hypothesis when the data is marginal.

\subsubsection{Likelihood ratio test}

The likelihood ratio test compares the likelihood of the ML parameters given the data under the null and alternative hypotheses.
It uses the test statistic
\begin{equation}
    \Lambda = 2\left(l_{\text{alt}}(\hat{\beta}_\text{alt}) - l_\text{null}(\hat{\beta}_\text{null})\right).
\end{equation}
In our model
\begin{align}
    \Lambda &= 2\left((\hat{\beta}_\text{alt} - \hat{\beta}_\text{null})\cdot T(x) - A_\text{alt}(\hat{\beta}_\text{alt}) + A_\text{null}(\hat{\beta}_\text{null})\right) \\
            &= 2(\hat{\beta}_\text{alt} - \hat{\beta}_\text{null}) \cdot T(x),
\end{align}
where the second equality comes from the fact that $A(\hat{\beta}) = \frac{\partial A}{\partial \beta_0}(\hat{\beta}) = T_0(x)$ under both models.
Like $Z$, $\Lambda$ is asymptotically standard normal under the null model.

Because it depends on $\hat{\beta}$, the likelihood ratio test shares the downsides of complexity with the Z-test.
On the other hand, it handles the first-observation problem more gracefully.
To see this, apply Eq.~\ref{eq:ec_scaling}, to get
\begin{align}
    \Lambda & \to 2 \left( \log(x_n / \lambda_n) x_n - \log(x_n / \lambtot) x_n \right) \\
            & = 2 \log(\lambtot / \lambda_n) x_n \\
            & \approx 2 \log(n) x_n,
\end{align}
which is finite.

\subsubsection{Score test}

The score test is an approximation to the likelihood ratio test that is simpler to calculate.
Its test statistic is based on the gradient of the log-likelihood (sometimes known as the `score'):
\begin{align}
    S^2 & = {(\nabla l)}^T \mathcal{I}^{-1} \nabla l,
\end{align}
where all components are evaluated at the parameters of the null hypothesis.
Substituting our previous results, gives
\begin{align}
    S^2(x) & = \frac{{(T_0(x) - \lambtot e^{\hat{\beta}_0})}^2 + T_1^2(x) / \sigma_t^2}{\lambtot e^{\hat{\beta}_0}}. \\
           & = \frac{T_1^2(x)}{T_0(x) \sigma_t^2}
\end{align}
Under the null hypothesis $S^2$ is asymptotically $\chi$-squared distributed with one degree of freedom so that $S$ is standard Gaussian.

The score test has the advantage that its test statistic can be computed directly from the count data without fitting the model.
This makes it promising as an initial screening step to rule out k-mers that have very little evidence of exponential growth.
It also makes it faster to simulate the distribution of the statistic under different hypotheses.

In the first-observation case, we have a finite test statistic:
\begin{align}
    S^2 = \frac{{(t_n - \bar{t})}^2}{\sigma_t^2} x_n.
\end{align}
Note that unlike the likelihood ratio statistic, this does not depend directly on the number of sampled points.
In a sense, the score test forgets this information by looking only at the aggregate sufficient statistics.
In doing so, it is more conservative about this case than the likelihood ratio test.
($S$ also scales with $\sqrt{x_n}$, unlike $\Lambda$, which is proportional to $x_n$, so the score test is more conservative about larger counts at the first observation.)

\subsubsection{Possible follow-up work on hypothesis tests}

It may be worthwhile to follow up on the above section with a more thorough (and largely numerical) analysis of the merits of the different tests and their test statistics.
Possible issues to investigate:
\begin{enumerate}
    \item Calibration of asymptotic p-values: how do the true distributions of the test statistics compare to their asymptotic Gaussian distributions? Do any converge to Gaussian faster than the others?
    \item Power: How good is the test at detecting growth as a function of growth rate and false-positive rate (e.g., precision-recall curves)?
    \item Robustness: How sensitive is the test to model mis-specification (e.g., over-dispersion of counts, periodic fluctuations)?
    \item Extensibility: How easy is it to extend the test to deal with complications like the serial acquisition of data?
\end{enumerate}

\subsection{Uniform-sampling approximation}

Up until this point, we have worked with arbitrary $\lambda$ and sampling times.
It can be useful for intuition to consider the case of evenly spaced samples with equal intensity $\lambtot / n$ on the interval $[-\Delta t / 2, \Delta t / 2]$.
We'll further assume that we have dense sampling so that $n \ll 1$.
To simplify the expressions we'll neglect terms of order $n^{-1}$ and higher.
[Note: we can compute more terms in the asympotic series in $n^{-1}$, to capture the dependence on $n$, but I don't think it's worth it.]
With these assumptions, we can make the substitutions:
\begin{align}
    \frac{2 t_i}{\Delta t} & \to t \\
    \bar{t} & \to 0 \\
    \frac{\beta_1 \Delta t}{2} & \to \beta_1 \\
    \sum_{i=1}^{n} \lambda_i & \to \int_{-1}^{1} dt \frac{\lambtot}{2}
\end{align}
Note that we're now measuring time in terms of the total interval of our sampling, so that $\beta_1$ is a dimensionless quantity.
These substitutions send the log-partition function to
\begin{align}
    A(\beta) & \to \int_{-1}^{1} dt \frac{\lambtot}{2} e^{\beta_0 + \beta_1 t} \\
             & = \lambtot e^{\beta_0} \frac{\sinh(\beta_1)}{\beta_1}.
\end{align}
We now have the gradient:
\begin{align}
    \frac{\partial A}{\partial \beta_0} & = A \\
    \frac{\partial A}{\partial \beta_1} & = \lambtot e^{\beta_0} \frac{\beta_1 \cosh \beta_1 - \sinh \beta_1}{\beta_1^2}.
\end{align}
This yields the normalized max likelihood function for $\hat{\beta}_1$
\begin{align}
    \frac{T_1(x)}{T_0(x)} = \coth \hat{\beta}_1 - \frac{1}{\hat{\beta}_1}
    \begin{cases}
        = \frac{\hat{\beta}_1}{3} - \frac{\hat{\beta}_1^3}{45} + O(\hat{\beta}_1^5) \\
        \sim 1 - \frac{1}{\hat{\beta}_1}, \hat{\beta}_1 \to \infty.
    \end{cases}
\end{align}

Finally, we have the Fisher information:
\begin{align}
    \mathcal{I}(\beta) = \lambtot e^{\beta_0}
    \begin{pmatrix}
        \frac{\sinh \beta_1}{\beta_1} & \frac{\beta_1 \cosh \beta_1 - \sinh \beta_1}{\beta_1^2} \\
        \frac{\beta_1 \cosh \beta_1 - \sinh \beta_1}{\beta_1^2} &
        \frac{(\beta_1^2 + 2) \sinh \beta_1 - 2 \beta_1 \cosh \beta_1}{\beta_1^3}. \\
    \end{pmatrix}
\end{align}

Some algebra will show that these results agree with the general $\lambda$ results above.

\section{Bayesian inference}

\subsection{The conjugate prior and update rule}

\subsection{Selection and interpretation of hyperparameters}

\subsection{Point estimates and uncertainty}

\subsection{Regularization of the first-observation problem}

\subsection{Multiple hypothesis testing}

\end{document}

Let $a(t)$ be the abundance of an exponentially-growing pathogen over time:
\begin{align}
    a(t) = \exp(\beta_0 + \beta_1 t)
\end{align}
for $t \in \left[ 0, T\right]$.
Suppose we take $n$ measurements at times $\left\{t_i\right\}$ with exposures $\left\{\lambda_i\right\}$ for $i = 1, \ldots, n-1$.
The exposures represent a conversion factor between the abundance and the expected number of counts and lump together the sampling intensity and properties of the sequencing protocol such as taxonomic bias.
We model the vector of observed counts $\vec{k}$ as independent Poisson random variables:
\begin{align}
    k_i \sim \text{Poisson}(\lambda_i a(t_i)).
\end{align}
We use Poisson regression to estimate the parameters $(\beta_0, \beta_1)$.
This is equivalent to maximizing the log-likelihood:
\begin{align}
    l(\beta_0, \beta_1; \vec{k}, \vec{\lambda})
    & = \log \prod_{i=0}^{n-1} \frac{{(\lambda_i a(t_i))}^{k_i}}{k!} \exp(\lambda_i a(t_i))\\
    & = \sum_{i=0}^{n-1} \left[k_i(\beta_0 + \beta_1 t_i) - \lambda_i e^{\beta_0 + \beta_1 t_i} \right] + \text{const.}
\end{align}

In the limit where we have a lot of data (query what exactly this means in this context), the covariance matrix of the maximum likelihood parameter estimates $(\hat{\beta_0}, \hat{\beta_1})$ converges to the inverse of the Fisher information matrix,
\begin{equation}
    \mathcal{I}_{i, j} = - \mathbb{E} \left[ \frac{\partial^2 l}{\partial \beta_i \partial \beta_j} \middle| \beta_0, \beta_1 \right].
\end{equation}
Differentiating the log likelihood twice gives us:
\begin{align}
    \frac{\partial^2 l}{\partial \beta_0^2}
    &= - \sum_{i=0}^{n-1} \lambda_i e^{\beta_0 + \beta_1 t} \\
    \frac{\partial^2 l}{\partial \beta_0 \partial \beta_1}
    &= - \sum_{i=0}^{n-1} \lambda_i t_i e^{\beta_0 + \beta_1 t} \\
    \frac{\partial^2 l}{\partial \beta_1^2}
    &= - \sum_{i=0}^{n-1} \lambda_i t_i^2 e^{\beta_0 + \beta_1 t}.
\end{align}
If we take evenly spaced samples with equal exposure so that $t_i = T(i/n-1)$ and $\lambda_i = \lambda / n$, with total exposure $\lambda$, we have
\begin{equation}
    \mathcal{I} = \lambda e^{\beta_0}
    \begin{pmatrix}
        S_0 & T S_1 \\
        T S_1 & T^2 S_2
    \end{pmatrix}
\end{equation}
where
\begin{align}
    S_j = n^{-1} \sum_{i=0}^{n-1} {\left(\frac{i}{n-1}\right)}^j \exp\left(\beta_1 T \frac{i}{n-1}\right)
\end{align}

Because $\mathcal{I}$ is a $2 \times 2$ matrix, we can invert it to get the asymptotic covariance matrix:
\begin{equation}
    {\mathcal{I}}^{-1} = \lambda^{-1} e^{-\beta_0} T^{-2} \frac{1}{S_0 S_2 - S_1^2}
    \begin{pmatrix}
        T^2 S_2 & -T S_1 \\
        -T S_1 & S_0
    \end{pmatrix}
\end{equation}
For exponential growth detection, we're most interested in
\begin{equation}
    \textrm{Var}(\hat{\beta_1}) = \lambda^{-1} e^{-\beta_0} T^{-2} \frac{S_0}{S_0 S_2 - S_1^2}.
    \label{eq:var}
\end{equation}

We can already learn some important things from Eq.~\ref{eq:var}:
\begin{enumerate}
    \item The variance of $\hat{\beta_1}$ is inversely proportional to the total sampling effort, $\lambda$, and to the initial abundance, $e^{\beta_0}$.
    \item The standard error is inversely proportional to the total time spanned by the sampling, $T$.
    \item The variance depends on the true growth rate through the dimensionless parameter $\beta_1 T$, which represents the log-fold increase in abundance over $T$.
\end{enumerate}

Next, we want to clarify the dependence of $\textrm{Var}(\hat{\beta_1})$ on the number of sampled timepoints, $n$, and on the log fold-increase $\beta_1 T$, which are opaque in Eq.~\ref{eq:var}.
First, we will get the exact equation for the situation with no growth ($\beta_1 = 0$).
Then, we'll look at the general case and study the large $n$ behavior with an asymptotic series in $n^{-1}$.

When $\beta_1 = 0$, the sums become simple:
\begin{align}
    S_0 &= 1 \\
    S_1 &= \frac{n-1}{2} \\
    S_2 &= \frac{(n-1)(2n-1)}{6}
\end{align}
so that we have
\begin{align}
    \textrm{Var}(\hat{\beta_1}) &= \lambda^{-1} e^{-\beta_0} \frac{12}{T^2} \left(\frac{n-1}{n+1}\right) \\
                                &\sim \lambda^{-1} e^{-\beta_0} \frac{12}{T^2} \left(1 - \frac{2}{n} + \frac{2}{n^2} + \cdots\right)
\end{align}
as $n \to \infty$.

Note that the variance converges to a constant as the number of samples increases (holding the total exposure constant).
This is intuitive because we expect diminishing returns to ever-finer temporal resolution.
Also note that the error of the first order approximation in $n^{-1}$ is already only 2\% once you have 10 sampled times, so for the full case, we won't bother with higher-order approximations.
